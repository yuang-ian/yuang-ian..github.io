<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuang Shi</title>
  
  <meta name="author" content="Yuang Shi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuang Shi (ÊñΩÂÆáÊòÇ)</name>
              </p>
              <p style="text-align:justify">
                I'm a second year Ph.D. Candidate at <a href="https://www.nus.edu.sg">National University of Singapore</a> ü¶Å advised by <a href="https://www.comp.nus.edu.sg/cs/people/ooiwt/">Prof. Wei Tsang Ooi</a>, where I work on Networking and Multimedia Systems.
              </p>
              <p style="text-align:justify">
                I received my M.Comp Degree in 2022 from <a href="https://www.nus.edu.sg">National University of Singapore</a>, where I did research on human activity recognition in-the-wild. I received my B. Eng. Degree in 2021 from <a href="https://en.wikipedia.org/wiki/Sichuan_University">Sichuan University</a> üêº in China, where I worked on Medical Image Processing.
              </p>
              <p style="text-align:justify">
                I'm also a super cat üê± lover. My ultimate dream during my PhD journey is to make friends with all the cats in Singapore. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yuangshi@u.nus.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=ytwibHUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/Shiyuang-scu">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SHIyuang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SHIyuang_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                  As a PhD student, I mainly focus on volumetric video compression, evaluation, and streaming.
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/QE_presentation.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a >
                  <papertitle>Human-Centric Bandwidth-Efficient Volumetric Video Streaming</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>
                <br>
                <em>Qualification Exam Presentation. </em>Oct 2023.
                <br>
                <a href="https://raw.githubusercontent.com/yuang-ian/yuang-ian.github.io/main/images/PhD_QE_Presentation.pdf" target="_blank">PDF (6 MB without video demo)</a> /
                <a href="https://docs.google.com/presentation/d/1b64MB1p5muSJSB1uoDX7f6-6ZB47-mR6/edit?usp=sharing&ouid=116756645024351581996&rtpof=true&sd=true" target="_blank">PowerPoint (150 MB)</a>
                <p>This is my QE presentation, where I introduced the existing challenges and opportunities, and my contributions, ongoing works, and future plan in volumetric video streaming.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmsys23_teaser_ref.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590981">
                  <papertitle>Enabling Low Bit-Rate MPEG V-PCC-encoded Volumetric Video Streaming with 3D Sub-sampling</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Pranav Venkatram, Yifan Ding, Wei Tsang Ooi
                <br>
                <em>Proceedings of the 14th ACM Multimedia Systems Conference. </em>ACM, 2023.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3590981">paper</a> 
                <p>We show that it is possible to improve the quality of V-PCC encoded point clouds at low bit-rate by exploiting redundant information among the points in the 3D domain.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mmsys23_dataset.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3592546">
                  <papertitle>A Dynamic 3D Point Cloud Dataset for Immersive Applications</papertitle>
                </a>
                <br>
                Yuan-Chun Sun, I-Chun Huang, <strong>Yuang Shi</strong>, Wei Tsang Ooi, Chun-Ying Huang, Cheng-Hsin Hsu
                <br>
                <em>Proceedings of the 14th ACM Multimedia Systems Conference. </em>ACM, 2023.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3587819.3592546">paper</a> 
                <p>We present an open dynamic 3D point cloud dataset with motion ground truth, which can be used by researchers who need temporal information across frames, e.g., motion estimation.</p>
                <br><br>
              </td>
            </tr>	

          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p>
                  When I was a Master student, my dissertation is about human activity recognition in-the-wild. 
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/shapecnf_architecture.png" alt="clean-usnob" width="160" height="70">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9767353">
                  <papertitle>Shape-Based Conditional Neural Field for Wrist-Worn Change-Point Detection</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, Varsha Suresh, Wei Tsang Ooi
                <br>
                <em>2022 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops). </em>IEEE, 2022.
                <br>
                <a href="https://drive.google.com/file/d/1cDNLQkChUhzaJeazOrpSmHLusLIt5JVz/view?usp=sharing">paper</a> /
                <a href="https://github.com/Shiyuang-scu/ShapeCNF">code</a>
                <p>ShapeCNF is a simple, fast, and accurate change-point detection method which uses shape-based features to model the patterns and a conditional neural field to model the temporal correlations among the time regions.</p>
                <br><br>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <p>
                  During my undergraduate, I spent most of my time on medical image analysis.
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/npc_seg.png" alt="clean-usnob" width="160" height="110">
              </td>
              <td width="75%" valign="middle">
                <a href="https://doi.org/10.1016/j.knosys.2023.110598">
                  <papertitle>Uncertainty-weighted and Relation-driven Consistency Training for Semi-supervised Head-and-Neck Tumor Segmentation</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, et al.
                <br>
                <em>Knowledge-based Systems </em> (2023): 110598.
                <br>
                <!-- <a href="https://drive.google.com/file/d/1hG1PZC4ZAqyZw0hJRDqGrzqBxcDhVTHz/view?usp=sharing">paper</a>  -->
                <!-- / <a href="https://github.com/Shiyuang-scu/ASMFS">code</a> -->
                <p>We propose a consistency training framework for semi-supervised NPC segmentation, which includes an Uncertainty-weighted Prediction Consistency Training (UPCT) strategy and a Relation-driven Consistency Training (RCT) strategy.</p>
                <br><br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ASMFS.png" alt="clean-usnob" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320322000474">
                  <papertitle>ASMFS: Adaptive-Similarity-based Multi-modality Feature Selection for Classification of Alzheimer's Disease</papertitle>
                </a>
                <br>
                <strong>Yuang Shi</strong>, et al.
                <br>
                <em>Pattern Recognition </em>126 (2022): 108566.
                <br>
                <a href="https://drive.google.com/file/d/1hG1PZC4ZAqyZw0hJRDqGrzqBxcDhVTHz/view?usp=sharing">paper</a> /
                <a href="https://github.com/Shiyuang-scu/ASMFS">code</a>
                <p>ASMFS is a novel multi-modal feature selection method for classification of Alzheumer's Disease, which performs adaptive similarity learning and feature selection simultaneously.</p>
                <br><br>
              </td>
            </tr>
          </tbody>
        </table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/volunteer.jpg" alt="clean-usnob" width="160" height="120">
            </td>
            <td width="75%" valign="center">
              Student Volunteer (SIGMM Student Travel Award), The 14th ACM Multimedia Systems Conference (MMSys), 2023
              <br><br>
              Student Volunteer, The 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2022
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/panda.jpg" alt="cs3244" width="160" height="120">
            </td>
            <td width="75%" valign="center">
              Graduate Tutor & Teaching Assistant, CS3244 Machine Learning, Spring 2023
              <br><br>
              Graduate Tutor & Teaching Assistant, CS3244 Machine Learning, Fall 2022
              <br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/photograph.jpg" alt="photo" width="160" height="120">
            </td>
            <td width="75%" valign="center">
              I love photography. Have a look at some of my <a href="https://flic.kr/ps/3WM9SL"> photos</a>
              <br><br>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                This template comes from Jon Barron's public academic <a href="https://jonbarron.info/">website</a>. ‚ù§Ô∏è
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
